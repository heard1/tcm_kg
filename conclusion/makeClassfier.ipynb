{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import json\n",
    "\n",
    "coon = pymysql.connect(\n",
    "    host = '10.15.82.58',user = 'root',passwd = '123',\n",
    "    port = 3306,db = 'tcm',charset = 'utf8'\n",
    "    )\n",
    "source = coon.cursor()\n",
    "source.execute(\"select json from qa_conclusion\")\n",
    "tem = source.fetchall()\n",
    "source.close()\n",
    "coon.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yufang = []\n",
    "jiancha = []\n",
    "yaowu = []\n",
    "zhenjiu = []\n",
    "tuina = []\n",
    "huoguan = []\n",
    "shoushu = []\n",
    "pianfang = []\n",
    "yangshen = []\n",
    "\n",
    "buliangfanyin = []\n",
    "paozhi = []\n",
    "xiangguanjibin = []\n",
    "yongfa = []\n",
    "gongxiao = []\n",
    "\n",
    "for single in tem:\n",
    "    data = json.loads(single[0])\n",
    "    for line in data['children']:\n",
    "        if line['name'] == '预防':\n",
    "            for sen in line['children']:\n",
    "                yufang.append(sen['name'])\n",
    "        elif line['name'] == '检查':\n",
    "            for sen in line['children']:\n",
    "                jiancha.append(sen['name'])\n",
    "        elif line['name'] == '治疗':\n",
    "            for nex in line['children']:\n",
    "                if nex['name'] == '药物':\n",
    "                    for nexx in nex['children']:\n",
    "                        yaowu.append(nexx['name'])\n",
    "                elif nex['name'] == '针灸':\n",
    "                    for nexx in nex['children']:\n",
    "                        zhenjiu.append(nexx['name'])\n",
    "                elif nex['name'] == '推拿':\n",
    "                    for nexx in nex['children']:\n",
    "                        tuina.append(nexx['name'])\n",
    "                elif nex['name'] == '火罐':\n",
    "                    for nexx in nex['children']:\n",
    "                        huoguan.append(nexx['name'])\n",
    "                elif nex['name'] == '手术':\n",
    "                    for nexx in nex['children']:\n",
    "                        shoushu.append(nexx['name'])\n",
    "                elif nex['name'] == '偏方':\n",
    "                    for nexx in nex['children']:\n",
    "                        pianfang.append(nexx['name'])\n",
    "        elif line['name'] == '养生':\n",
    "            for sen in line['children']:\n",
    "                yangshen.append(sen['name'])\n",
    "        elif line['name'] == '不良反应':\n",
    "            for sen in line['children']:\n",
    "                buliangfanyin.append(sen['name'])\n",
    "        elif line['name'] == '炮制':\n",
    "            for sen in line['children']:\n",
    "                paozhi.append(sen['name'])\n",
    "        elif line['name'] == '相关疾病':\n",
    "            for sen in line['children']:\n",
    "                xiangguanjibin.append(sen['name'])\n",
    "        elif line['name'] == '用法':\n",
    "            for sen in line['children']:\n",
    "                yongfa.append(sen['name'])\n",
    "        elif line['name'] == '功效':\n",
    "            for sen in line['children']:\n",
    "                gongxiao.append(sen['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import jieba\n",
    "import re\n",
    "import keras\n",
    "from keras import Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Dense, Flatten, Embedding, Dropout, concatenate, Bidirectional, LSTM, GRU, Reshape\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "def resample(data_dict,sample_size,seed,test_rate=0.1):\n",
    "    \"\"\"\n",
    "    数据重采样\n",
    "    :param data_dict: 输入数据dict类型：{label_1:[text_11,text_12,...,text_1j,...],label_2:[...],...,label_i:[...],...}\n",
    "    :param sample_size: 每类样本容量：超过的样本类进行下采样，不足的样本类进行上采样\n",
    "    :param seed: 随机种子\n",
    "    :param test_rate: 测试集比例\n",
    "    :return: train：训练集；test：测试集\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    train = []\n",
    "    test = []\n",
    "    for label in data_dict:\n",
    "        #先抽取测试集\n",
    "        num_data = len(data_dict[label])\n",
    "        test_num_data = max(int(num_data // (1/test_rate)),1)\n",
    "        for i in range(test_num_data):\n",
    "            test_data = np.random.choice(data_dict[label],1)\n",
    "            test_index = np.where(data_dict[label]==test_data)\n",
    "            if test_num_data > 1:\n",
    "                data_dict[label] = np.delete(data_dict[label],test_index)\n",
    "            test.append([test_data[0],label])\n",
    "        #再抽取训练集\n",
    "        if len(data_dict[label]) >= sample_size:\n",
    "            #下采样(不能重复，要用random.sample)\n",
    "            train_data = random.sample(list(data_dict[label]),sample_size)\n",
    "            train.extend([[train_data_i,label] for train_data_i in train_data])\n",
    "        else:\n",
    "            #上采样（有重复，要用np.random.choice+原数据）\n",
    "            train_data = list(data_dict[label])\n",
    "            train_data_extend = np.random.choice(train_data,sample_size-len(train_data)).tolist()\n",
    "            train_data.extend(train_data_extend)\n",
    "            train.extend([[train_data_i,label] for train_data_i in train_data])\n",
    "    return train,test\n",
    "\n",
    "regex = re.compile(u'[^\\u4E00-\\u9FA5|0-9a-zA-Z]')\n",
    "def remove_punctuation(s):\n",
    "    \"\"\"\n",
    "    文本标准化：仅保留中文字符、英文字符和数字字符\n",
    "    :param s: 输入文本（中文文本要求进行分词处理）\n",
    "    :return: s_：标准化的文本\n",
    "    \"\"\"\n",
    "    s_ = regex.sub('', s)\n",
    "    return s_\n",
    "\n",
    "def text_tokenizer(texts):\n",
    "    \"\"\"\n",
    "    文本分词+标准化\n",
    "    :param texts: 输入文本list类型：[text_1,text_2,...,text_i,...]\n",
    "    :return: 标准化后的分词文本\n",
    "    \"\"\"\n",
    "    return [jieba.lcut(remove_punctuation(text)) for text in texts]\n",
    "\n",
    "def BiGRU(train,test,val,output_categories,num_words,maxlen,embedding_dim,\n",
    "          gru_units=256,dropout=0.2,recurrent_dropout=0.2,\n",
    "          batch_size=512,epochs=10,verbose=1):\n",
    "    \"\"\"\n",
    "    TextRNN模型：双向GRU\n",
    "    :param train: 训练数据，格式：(data,label)\n",
    "    :param test: 测试数据，格式：(data,label)\n",
    "    :param val: 验证数据，格式：(data,label)\n",
    "    :param output_categories: 分类类别数\n",
    "    :param num_words: 限制top_n词数（文本内仅计算top_n的词向量）\n",
    "    :param maxlen: 最大句长限制（计算基本单元：词组）\n",
    "    :param embedding_dim: 词向量维度\n",
    "    :param callbacks: 每个epoch进行回调计算的指标\n",
    "    :param gru_units: GRU层神经元数量\n",
    "    :param dropout:\n",
    "    :param recurrent_dropout:\n",
    "    :param batch_size:\n",
    "    :param epochs:\n",
    "    :param verbose:\n",
    "    :return: model：模型；train_acc：训练精度；val_acc：验证精度；test_acc：测试精度\n",
    "    \"\"\"\n",
    "    #一、数据准备\n",
    "    train_data,train_label = train\n",
    "    test_data,test_label = test\n",
    "    val_data,val_label = val\n",
    "    #一、网络构建\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embedding = Embedding(input_dim=num_words,input_length=maxlen,output_dim=embedding_dim,trainable=True)(inputs)\n",
    "    bigru = Bidirectional(GRU(units=gru_units,dropout=dropout,recurrent_dropout=recurrent_dropout,return_sequences=True))(embedding)\n",
    "    bigru = Bidirectional(GRU(units=gru_units,dropout=dropout,recurrent_dropout=recurrent_dropout,activation='relu'))(bigru)\n",
    "    outputs = Dense(output_categories,activation='softmax')(bigru)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    model.summary()\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-8),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    #二、模型训练及指标返回\n",
    "    history = model.fit(train_data,train_label,batch_size=batch_size,validation_data=(val_data,val_label),epochs=epochs,verbose=verbose)\n",
    "    train_acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    test_loss,test_acc = model.evaluate(test_data,test_label)\n",
    "    return model,train_acc,val_acc,test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n",
      "Test::   0%|          | 0/5000 [00:00<?, ?it/s] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.938 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Test:: 100%|██████████| 5000/5000 [00:01<00:00, 3422.99it/s]\n",
      "\n",
      "  0%|          | 1/1324 [00:01<32:11,  1.46s/it]\u001b[A\n",
      "100%|██████████| 1324/1324 [00:01<00:00, 831.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'不良反应': 0, '炮制': 1, '相关疾病': 2, '用法': 3, '功效': 4}\n",
      "{0: '不良反应', 1: '炮制', 2: '相关疾病', 3: '用法', 4: '功效'}\n",
      "['不良反应', '炮制', '相关疾病', '用法', '功效']\n"
     ]
    }
   ],
   "source": [
    "    # 1. 数据加载\n",
    "    data=[]\n",
    "    label=[]\n",
    "#     data.extend(yufang)\n",
    "#     data.extend(jiancha)\n",
    "#     data.extend(yaowu)\n",
    "#     data.extend(zhenjiu)\n",
    "#     data.extend(tuina)\n",
    "#     data.extend(huoguan)\n",
    "#     data.extend(shoushu)\n",
    "#     data.extend(pianfang)\n",
    "#     data.extend(yangshen)\n",
    "    data.extend(buliangfanyin)\n",
    "    data.extend(paozhi)\n",
    "    data.extend(xiangguanjibin)\n",
    "    data.extend(yongfa)\n",
    "    data.extend(gongxiao)\n",
    "#     for i in range(len(yufang)):\n",
    "#         label.append(\"预防\")\n",
    "#     for i in range(len(jiancha)):\n",
    "#         label.append(\"检查\")\n",
    "#     for i in range(len(yaowu)):\n",
    "#         label.append(\"药物\")\n",
    "#     for i in range(len(zhenjiu)):\n",
    "#         label.append(\"针灸\")\n",
    "#     for i in range(len(tuina)):\n",
    "#         label.append(\"推拿\")\n",
    "#     for i in range(len(huoguan)):\n",
    "#         label.append(\"火罐\")\n",
    "#     for i in range(len(shoushu)):\n",
    "#         label.append(\"手术\")\n",
    "#     for i in range(len(pianfang)):\n",
    "#         label.append(\"偏方\")\n",
    "#     for i in range(len(yangshen)):\n",
    "#         label.append(\"养生\")\n",
    "    for i in range(len(buliangfanyin)):\n",
    "        label.append(\"不良反应\")\n",
    "    for i in range(len(paozhi)):\n",
    "        label.append(\"炮制\")\n",
    "    for i in range(len(xiangguanjibin)):\n",
    "        label.append(\"相关疾病\")\n",
    "    for i in range(len(yongfa)):\n",
    "        label.append(\"用法\")\n",
    "    for i in range(len(gongxiao)):\n",
    "        label.append(\"功效\")\n",
    "\n",
    "    data_dict = {}\n",
    "    for data_i,label_i in zip(data,label):\n",
    "        data_dict.setdefault(label_i,[]).append(data_i)\n",
    "\n",
    "    sample_size = 1000\n",
    "    seed = 0\n",
    "    test_rate = 0.1\n",
    "    train,test = resample(data_dict=data_dict,sample_size=sample_size,seed=seed,test_rate=test_rate)\n",
    "\n",
    "    # 2. 数据处理\n",
    "    tqdm_train = tqdm(train)\n",
    "    tqdm_test = tqdm(test)\n",
    "    tqdm_train.set_description(\"Train:\")\n",
    "    tqdm_train.set_description(\"Test:\")\n",
    "    train_label = []\n",
    "    train_data = []\n",
    "    test_label = []\n",
    "    test_data = []\n",
    "    try:\n",
    "        for item in tqdm_train:\n",
    "            data = item[0]\n",
    "            label = item[1]\n",
    "            train_label.append(label)\n",
    "            train_data.append(text_tokenizer([data])[0])\n",
    "    except KeyboardInterrupt:\n",
    "        tqdm.close()\n",
    "        raise\n",
    "    try:\n",
    "        for item in tqdm_test:\n",
    "            data = item[0]\n",
    "            label = item[1]\n",
    "            test_label.append(label)\n",
    "            test_data.append(text_tokenizer([data])[0])\n",
    "    except KeyboardInterrupt:\n",
    "        tqdm.close()\n",
    "        raise\n",
    "\n",
    "    label_set = []\n",
    "    for label in train_label:\n",
    "        if label not in label_set:\n",
    "            label_set.append(label)\n",
    "\n",
    "    label_convert2 = dict([[item,label_set.index(item)] for item in label_set])\n",
    "    print(label_convert2)\n",
    "    label_reconvert2 = dict([(label_convert2[key],key) for key in label_convert2])\n",
    "    print(label_reconvert2)\n",
    "\n",
    "    train_label = [label_convert2[item] for item in train_label]\n",
    "    test_label = [label_convert2[item] for item in test_label]\n",
    "    train_label = to_categorical(train_label)\n",
    "    test_label = to_categorical(test_label)\n",
    "\n",
    "    # 3. 文本序列化\n",
    "    num_words = 10000\n",
    "    maxlen = 25\n",
    "    embedding_dim = 128\n",
    "    output_categories = len(label_convert2)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(train_data)\n",
    "    train_data = tokenizer.texts_to_sequences(train_data)\n",
    "    test_data = tokenizer.texts_to_sequences(test_data)\n",
    "    train_data = pad_sequences(train_data,maxlen=maxlen)\n",
    "    test_data = pad_sequences(test_data,maxlen=maxlen)\n",
    "\n",
    "    # 4. 输出指标\n",
    "    target_names = [label_reconvert2[i] for i in range(len(label_reconvert2))]\n",
    "    print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 25, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 25, 512)           591360    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 512)               1181184   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 3,055,109\n",
      "Trainable params: 3,055,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /data2/home/zju/workspace/bx/dead/anaconda3/envs/lsc/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 5000 samples, validate on 1324 samples\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 1.5707 - acc: 0.3582 - val_loss: 1.5154 - val_acc: 0.4403\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 1.2695 - acc: 0.5342 - val_loss: 1.3097 - val_acc: 0.4705\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.9679 - acc: 0.6566 - val_loss: 1.1704 - val_acc: 0.5589\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.7466 - acc: 0.7348 - val_loss: 1.0533 - val_acc: 0.6201\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.5809 - acc: 0.7826 - val_loss: 0.9713 - val_acc: 0.6427\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.4869 - acc: 0.8064 - val_loss: 0.9235 - val_acc: 0.6412\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.4186 - acc: 0.8284 - val_loss: 0.9356 - val_acc: 0.6458\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.3817 - acc: 0.8366 - val_loss: 0.9484 - val_acc: 0.6397\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.3440 - acc: 0.8518 - val_loss: 0.9423 - val_acc: 0.6329\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.3197 - acc: 0.8568 - val_loss: 0.9606 - val_acc: 0.6299\n",
      "1324/1324 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# 二、模型训练\n",
    "\n",
    "    # 1. 设置基本超参数（未调参）\n",
    "\n",
    "basic_hyper_parameters = {\n",
    "        'train': (train_data, train_label),\n",
    "        'test': (test_data, test_label),\n",
    "        'val': (test_data, test_label),\n",
    "        'output_categories': output_categories,\n",
    "        'num_words': num_words,\n",
    "        'maxlen': maxlen,\n",
    "        'embedding_dim': embedding_dim\n",
    "    }\n",
    "bigruY, bigru_train_acc, bigru_val_acc, bigru_test_acc = BiGRU(**basic_hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9000 [00:00<?, ?it/s]\n",
      "Test:: 100%|██████████| 9000/9000 [00:00<00:00, 10093.13it/s]\n",
      "\n",
      "  0%|          | 1/1778 [00:00<26:19,  1.13it/s]\u001b[A\n",
      "100%|██████████| 1778/1778 [00:01<00:00, 1642.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'预防': 0, '检查': 1, '药物': 2, '针灸': 3, '推拿': 4, '火罐': 5, '手术': 6, '偏方': 7, '养生': 8}\n",
      "{0: '预防', 1: '检查', 2: '药物', 3: '针灸', 4: '推拿', 5: '火罐', 6: '手术', 7: '偏方', 8: '养生'}\n",
      "['预防', '检查', '药物', '针灸', '推拿', '火罐', '手术', '偏方', '养生']\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 25, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 25, 512)           591360    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 512)               1181184   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 4617      \n",
      "=================================================================\n",
      "Total params: 3,057,161\n",
      "Trainable params: 3,057,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9000 samples, validate on 1778 samples\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 14s 2ms/step - loss: 2.0527 - acc: 0.2768 - val_loss: 1.9390 - val_acc: 0.4747\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 1.2284 - acc: 0.5966 - val_loss: 1.4847 - val_acc: 0.6569\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5600 - acc: 0.8300 - val_loss: 1.1587 - val_acc: 0.7503\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.2950 - acc: 0.9218 - val_loss: 0.8974 - val_acc: 0.8037\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1848 - acc: 0.9496 - val_loss: 0.7855 - val_acc: 0.8178\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1415 - acc: 0.9603 - val_loss: 0.7560 - val_acc: 0.8189\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.1159 - acc: 0.9667 - val_loss: 0.7120 - val_acc: 0.8088\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.1031 - acc: 0.9696 - val_loss: 0.6821 - val_acc: 0.8273\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.0998 - acc: 0.9696 - val_loss: 0.6644 - val_acc: 0.8189\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0912 - acc: 0.9687 - val_loss: 0.6574 - val_acc: 0.8161\n",
      "1778/1778 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "    # 1. 数据加载\n",
    "    data=[]\n",
    "    label=[]\n",
    "    data.extend(yufang)\n",
    "    data.extend(jiancha)\n",
    "    data.extend(yaowu)\n",
    "    data.extend(zhenjiu)\n",
    "    data.extend(tuina)\n",
    "    data.extend(huoguan)\n",
    "    data.extend(shoushu)\n",
    "    data.extend(pianfang)\n",
    "    data.extend(yangshen)\n",
    "\n",
    "    for i in range(len(yufang)):\n",
    "        label.append(\"预防\")\n",
    "    for i in range(len(jiancha)):\n",
    "        label.append(\"检查\")\n",
    "    for i in range(len(yaowu)):\n",
    "        label.append(\"药物\")\n",
    "    for i in range(len(zhenjiu)):\n",
    "        label.append(\"针灸\")\n",
    "    for i in range(len(tuina)):\n",
    "        label.append(\"推拿\")\n",
    "    for i in range(len(huoguan)):\n",
    "        label.append(\"火罐\")\n",
    "    for i in range(len(shoushu)):\n",
    "        label.append(\"手术\")\n",
    "    for i in range(len(pianfang)):\n",
    "        label.append(\"偏方\")\n",
    "    for i in range(len(yangshen)):\n",
    "        label.append(\"养生\")\n",
    "\n",
    "\n",
    "    data_dict = {}\n",
    "    for data_i,label_i in zip(data,label):\n",
    "        data_dict.setdefault(label_i,[]).append(data_i)\n",
    "\n",
    "    sample_size = 1000\n",
    "    seed = 0\n",
    "    test_rate = 0.1\n",
    "    train,test = resample(data_dict=data_dict,sample_size=sample_size,seed=seed,test_rate=test_rate)\n",
    "\n",
    "    # 2. 数据处理\n",
    "    tqdm_train = tqdm(train)\n",
    "    tqdm_test = tqdm(test)\n",
    "    tqdm_train.set_description(\"Train:\")\n",
    "    tqdm_train.set_description(\"Test:\")\n",
    "    train_label = []\n",
    "    train_data = []\n",
    "    test_label = []\n",
    "    test_data = []\n",
    "    try:\n",
    "        for item in tqdm_train:\n",
    "            data = item[0]\n",
    "            label = item[1]\n",
    "            train_label.append(label)\n",
    "            train_data.append(text_tokenizer([data])[0])\n",
    "    except KeyboardInterrupt:\n",
    "        tqdm.close()\n",
    "        raise\n",
    "    try:\n",
    "        for item in tqdm_test:\n",
    "            data = item[0]\n",
    "            label = item[1]\n",
    "            test_label.append(label)\n",
    "            test_data.append(text_tokenizer([data])[0])\n",
    "    except KeyboardInterrupt:\n",
    "        tqdm.close()\n",
    "        raise\n",
    "\n",
    "    label_set = []\n",
    "    for label in train_label:\n",
    "        if label not in label_set:\n",
    "            label_set.append(label)\n",
    "\n",
    "    label_convert = dict([[item,label_set.index(item)] for item in label_set])\n",
    "    print(label_convert)\n",
    "    label_reconvert = dict([(label_convert[key],key) for key in label_convert])\n",
    "    print(label_reconvert)\n",
    "\n",
    "    train_label = [label_convert[item] for item in train_label]\n",
    "    test_label = [label_convert[item] for item in test_label]\n",
    "    train_label = to_categorical(train_label)\n",
    "    test_label = to_categorical(test_label)\n",
    "\n",
    "    # 3. 文本序列化\n",
    "    num_words = 10000\n",
    "    maxlen = 25\n",
    "    embedding_dim = 128\n",
    "    output_categories = len(label_convert)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(train_data)\n",
    "    train_data = tokenizer.texts_to_sequences(train_data)\n",
    "    test_data = tokenizer.texts_to_sequences(test_data)\n",
    "    train_data = pad_sequences(train_data,maxlen=maxlen)\n",
    "    test_data = pad_sequences(test_data,maxlen=maxlen)\n",
    "\n",
    "    # 4. 输出指标\n",
    "    target_names = [label_reconvert[i] for i in range(len(label_reconvert))]\n",
    "    print(target_names)\n",
    "    # 二、模型训练\n",
    "\n",
    "    # 1. 设置基本超参数（未调参）\n",
    "\n",
    "    basic_hyper_parameters = {\n",
    "        'train': (train_data, train_label),\n",
    "        'test': (test_data, test_label),\n",
    "        'val': (test_data, test_label),\n",
    "        'output_categories': output_categories,\n",
    "        'num_words': num_words,\n",
    "        'maxlen': maxlen,\n",
    "        'embedding_dim': embedding_dim\n",
    "    }\n",
    "    bigruB, bigru_train_acc, bigru_val_acc, bigru_test_acc = BiGRU(**basic_hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('炮制', 0.48746264)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predictB(new):\n",
    "    new = text_tokenizer([new])\n",
    "    new = tokenizer.texts_to_sequences(new)\n",
    "    new = pad_sequences(new, maxlen=25)\n",
    "    tem = bigruB.predict(new)\n",
    "    return label_reconvert[tem.argmax()], tem.max()\n",
    "predictB(\"我这个病需要动手术吗\")\n",
    "def predictY(new):\n",
    "    new = text_tokenizer([new])\n",
    "    new = tokenizer.texts_to_sequences(new)\n",
    "    new = pad_sequences(new, maxlen=25)\n",
    "    tem = bigruY.predict(new)\n",
    "    return label_reconvert2[tem.argmax()], tem.max()\n",
    "predictY(\"这个药吃下去有效果吗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('手术', 0.8269756)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictB(\"我这个病需要动手术吗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '预防',\n",
       " 1: '检查',\n",
       " 2: '药物',\n",
       " 3: '针灸',\n",
       " 4: '推拿',\n",
       " 5: '火罐',\n",
       " 6: '手术',\n",
       " 7: '偏方',\n",
       " 8: '养生'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_reconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = set()\n",
    "import pymysql\n",
    "coon = pymysql.connect(\n",
    "    host = '10.15.82.50',user = 'root',passwd = '123',\n",
    "    port = 3306,db = 'lsc',charset = 'utf8'\n",
    "    )\n",
    "data = coon.cursor()\n",
    "data.execute(\"select entity,type from conc group by entity having count(*)>10\")\n",
    "tem = data.fetchall()\n",
    "for line in tem:\n",
    "    entities.add(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "coon2 = pymysql.connect(\n",
    "    host = '10.15.82.58',user = 'root',passwd = '123',\n",
    "    port = 3306,db = 'qa',charset = 'utf8'\n",
    "    )\n",
    "q2 = 'select qa_q from qa_new where qa_id = %s'\n",
    "cur2 = coon2.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"select id from conc where entity = %s\"\n",
    "for entityT in entities:\n",
    "    entity = entityT[0]\n",
    "    BY = entityT[1]\n",
    "    data.execute(q1,(entity))\n",
    "    for line in data.fetchall():\n",
    "        cur2.execute(q2,(line[0]))\n",
    "        sent = cur2.fetchall()[0][0][:150]\n",
    "        if BY == \"drug\":\n",
    "            if \"鉴别\" in sent or \"假冒\" in sent or \"识别\" in sent or \"区分\" in sent or \"混淆\" in sent:\n",
    "                pred = (\"鉴别\", 1)\n",
    "            else:\n",
    "                pred = predictY(sent)\n",
    "        else:\n",
    "            pred = predictB(sent)\n",
    "        if pred[1] >= 0.40:\n",
    "            query = 'insert into conc3(entity, question, type, id) values(%s, %s, %s, %s)'\n",
    "            data.execute(query,(entity, sent, pred[0], line[0]))\n",
    "            coon.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsc",
   "language": "python",
   "name": "lsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
